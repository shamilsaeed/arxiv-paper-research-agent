{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from semanticscholar import SemanticScholar\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from typing import List, Dict, Any\n",
    "from config import Config\n",
    "import requests\n",
    "from src.embed.vector_db import MilvusManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "sch = SemanticScholar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "  query = \"ti:\\\"Lightweight G-YOLOv11: Advancing Efficient Fracture Detection\\\"\",\n",
    "  max_results = 1,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in Pediatric Wrist X-rays\n",
      "2024-12-31 21:07:40+00:00\n",
      "eess.IV\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r.title)\n",
    "    print(r.published)\n",
    "    print(r.primary_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/pdf/2502.19414v1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arxiv.Result(entry_id='http://arxiv.org/abs/2502.19414v1', updated=datetime.datetime(2025, 2, 26, 18, 58, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 2, 26, 18, 58, 13, tzinfo=datetime.timezone.utc), title='Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation', authors=[arxiv.Result.Author('Shiven Sinha'), arxiv.Result.Author('Shashwat Goel'), arxiv.Result.Author('Ponnurangam Kumaraguru'), arxiv.Result.Author('Jonas Geiping'), arxiv.Result.Author('Matthias Bethge'), arxiv.Result.Author('Ameya Prabhu')], summary=\"There is growing excitement about the potential of Language Models (LMs) to\\naccelerate scientific discovery. Falsifying hypotheses is key to scientific\\nprogress, as it allows claims to be iteratively refined over time. This process\\nrequires significant researcher effort, reasoning, and ingenuity. Yet current\\nbenchmarks for LMs predominantly assess their ability to generate solutions\\nrather than challenge them. We advocate for developing benchmarks that evaluate\\nthis inverse capability - creating counterexamples for subtly incorrect\\nsolutions. To demonstrate this approach, we start with the domain of\\nalgorithmic problem solving, where counterexamples can be evaluated\\nautomatically using code execution. Specifically, we introduce REFUTE, a\\ndynamically updating benchmark that includes recent problems and incorrect\\nsubmissions from programming competitions, where human experts successfully\\nidentified counterexamples. Our analysis finds that the best reasoning agents,\\neven OpenAI o3-mini (high) with code execution feedback, can create\\ncounterexamples for only <9% of incorrect solutions in REFUTE, even though\\nratings indicate its ability to solve up to 48% of these problems from scratch.\\nWe hope our work spurs progress in evaluating and enhancing LMs' ability to\\nfalsify incorrect solutions - a capability that is crucial for both\\naccelerating research and making models self-improve through reliable\\nreflective reasoning.\", comment='Technical Report', journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2502.19414v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2502.19414v1', title='pdf', rel='related', content_type=None)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://arxiv.org/category_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"2412.19446\"  # Replace with the arXiv ID you're interested in\n",
    "paper = sch.get_paper(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x['numCitedBy']) # number of citations\n",
    "print(x['influentialCitationCount']) # number of influential citations\n",
    "print(x['citationVelocity']) # citation velocity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citation_info(arxiv_id):\n",
    "    base_url = \"https://api.semanticscholar.org/v1/paper/\"\n",
    "    url = f\"{base_url}arXiv:{arxiv_id}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('citationCount', 0), data.get('influentialCitationCount', 0), data.get('citationVelocity', 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a search for all CS papers from 2024\n",
    "search = arxiv.Search(\n",
    "    query=f\"cat:cs.* AND submittedDate:[2024 TO 2025]\",\n",
    "    max_results=1000,  # Fetch all papers\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:01, 86.68it/s]\n"
     ]
    }
   ],
   "source": [
    "papers = []\n",
    "chunk_count = 0\n",
    "results = client.results(search)\n",
    "\n",
    "for paper in tqdm(results):\n",
    "    try:\n",
    "        papers.append({\n",
    "            'title': paper.title,\n",
    "            'arxiv_id': paper.get_short_id(),\n",
    "            'category': paper.primary_category,\n",
    "            'summary': paper.summary,\n",
    "            'submitted_date': paper.published.strftime('%Y-%m-%d')\n",
    "        })\n",
    "        citation_count, influential_citation_count, citation_velocity = get_citation_info(paper.get_short_id())\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching paper {paper.get_short_id()}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Titans: Learning to Memorize at Test Time',\n",
       " 'arxiv_id': '2501.00663v1',\n",
       " 'category': 'cs.LG',\n",
       " 'summary': 'Over more than a decade there has been an extensive research effort on how to\\neffectively utilize recurrent models and attention. While recurrent models aim\\nto compress the data into a fixed-size memory (called hidden state), attention\\nallows attending to the entire context window, capturing the direct\\ndependencies of all tokens. This more accurate modeling of dependencies,\\nhowever, comes with a quadratic cost, limiting the model to a fixed-length\\ncontext. We present a new neural long-term memory module that learns to\\nmemorize historical context and helps attention to attend to the current\\ncontext while utilizing long past information. We show that this neural memory\\nhas the advantage of fast parallelizable training while maintaining a fast\\ninference. From a memory perspective, we argue that attention due to its\\nlimited context but accurate dependency modeling performs as a short-term\\nmemory, while neural memory due to its ability to memorize the data, acts as a\\nlong-term, more persistent, memory. Based on these two modules, we introduce a\\nnew family of architectures, called Titans, and present three variants to\\naddress how one can effectively incorporate memory into this architecture. Our\\nexperimental results on language modeling, common-sense reasoning, genomics,\\nand time series tasks show that Titans are more effective than Transformers and\\nrecent modern linear recurrent models. They further can effectively scale to\\nlarger than 2M context window size with higher accuracy in needle-in-haystack\\ntasks compared to baselines.',\n",
       " 'submitted_date': '2024-12-31'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 38 JSONL files:\n",
      "- machine_learning_2024.jsonl: 1359 papers\n",
      "- game_theory_2024.jsonl: 68 papers\n",
      "- computation_and_language_2024.jsonl: 1030 papers\n",
      "- neural_and_evolutionary_computing_2024.jsonl: 57 papers\n",
      "- information_theory_2024.jsonl: 125 papers\n",
      "- performance_2024.jsonl: 8 papers\n",
      "- networking_and_internet_2024.jsonl: 98 papers\n",
      "- logic_in_computer_science_2024.jsonl: 65 papers\n",
      "- robotics_2024.jsonl: 382 papers\n",
      "- computational_geometry_2024.jsonl: 25 papers\n",
      "- software_engineering_2024.jsonl: 196 papers\n",
      "- mathematical_software_2024.jsonl: 1 papers\n",
      "- computers_and_society_2024.jsonl: 155 papers\n",
      "- computational_engineering_2024.jsonl: 63 papers\n",
      "- multimedia_2024.jsonl: 21 papers\n",
      "- human_computer_interaction_2024.jsonl: 199 papers\n",
      "- information_retrieval_2024.jsonl: 147 papers\n",
      "- hardware_architecture_2024.jsonl: 54 papers\n",
      "- programming_languages_2024.jsonl: 28 papers\n",
      "- artificial_intelligence_2024.jsonl: 420 papers\n",
      "- data_structures_and_algorithms_2024.jsonl: 74 papers\n",
      "- sound_2024.jsonl: 103 papers\n",
      "- emerging_technologies_2024.jsonl: 20 papers\n",
      "- discrete_mathematics_2024.jsonl: 9 papers\n",
      "- unknown_2024.jsonl: 1390 papers\n",
      "- distributed_computing_2024.jsonl: 104 papers\n",
      "- computer_vision_2024.jsonl: 2066 papers\n",
      "- computational_complexity_2024.jsonl: 20 papers\n",
      "- operating_systems_2024.jsonl: 6 papers\n",
      "- digital_libraries_2024.jsonl: 20 papers\n",
      "- social_and_information_networks_2024.jsonl: 60 papers\n",
      "- multiagent_systems_2024.jsonl: 35 papers\n",
      "- graphics_2024.jsonl: 24 papers\n",
      "- cryptography_and_security_2024.jsonl: 283 papers\n",
      "- symbolic_computation_2024.jsonl: 7 papers\n",
      "- databases_2024.jsonl: 62 papers\n",
      "- formal_languages_2024.jsonl: 11 papers\n",
      "- other_computer_science_2024.jsonl: 5 papers\n",
      "\n",
      "Total papers across all files: 8800\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def analyze_jsonl_files(directory: str = \"data/raw\"):\n",
    "    \"\"\"Analyze JSONL files in directory\"\"\"\n",
    "    path = Path(directory)\n",
    "    jsonl_files = list(path.glob(\"*.jsonl\"))\n",
    "    \n",
    "    total_files = len(jsonl_files)\n",
    "    total_papers = 0\n",
    "    papers_by_file = {}\n",
    "    \n",
    "    for file in jsonl_files:\n",
    "        paper_count = sum(1 for _ in open(file))\n",
    "        papers_by_file[file.name] = paper_count\n",
    "        total_papers += paper_count\n",
    "    \n",
    "    print(f\"\\nFound {total_files} JSONL files:\")\n",
    "    for filename, count in papers_by_file.items():\n",
    "        print(f\"- {filename}: {count} papers\")\n",
    "        \n",
    "    print(f\"\\nTotal papers across all files: {total_papers}\")\n",
    "    \n",
    "    return total_files, total_papers\n",
    "\n",
    "total_files, total_papers = analyze_jsonl_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-paper-research-agent-YSyrqAjQ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
